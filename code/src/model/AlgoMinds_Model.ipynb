{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b94286c7-2a40-4af9-9b0c-6ff60a5d5dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\sahan\\anaconda3\\lib\\site-packages (2.2.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy in c:\\users\\sahan\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sahan\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\sahan\\anaconda3\\lib\\site-packages (0.12.3)\n",
      "Requirement already satisfied: transformers in c:\\users\\sahan\\anaconda3\\lib\\site-packages (4.50.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\sahan\\anaconda3\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: torch in c:\\users\\sahan\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy scikit-learn imbalanced-learn transformers datasets torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7085ed54-2132-4932-ad57-e35253a56b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0966a7a-0259-4678-8538-46fff7dde951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== CONFIG ==============\n",
    "CHUNK_SIZE = 50000\n",
    "DATA_DIR = \"\"\n",
    "OUTPUT_FILE = \"final_dataset.csv\"\n",
    "SYNTHETIC_DATA_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d602bbfc-df42-480d-a773-c1b104adadbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading datasets...\n",
      "✅ Datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============== 1. LOAD & PROCESS DATA ==============\n",
    "def load_csv_efficiently(file_path, usecols=None):\n",
    "    \"\"\"Loads a CSV in chunks to avoid MemoryError.\"\"\"\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(file_path, chunksize=CHUNK_SIZE, usecols=usecols, low_memory=False):\n",
    "        chunks.append(chunk)\n",
    "    return pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "print(\"🔄 Loading datasets...\")\n",
    "credit_card_df = load_csv_efficiently(os.path.join(DATA_DIR, \"creditcard.csv\"), usecols=[\"Amount\", \"Class\"])\n",
    "paysim_df = load_csv_efficiently(os.path.join(DATA_DIR, \"PS_20174392719_1491204439457_log.csv\"), usecols=[\"amount\", \"isFraud\"])\n",
    "transaction_df = load_csv_efficiently(os.path.join(DATA_DIR, \"train_transaction.csv\"), usecols=[\"TransactionID\", \"isFraud\", \"TransactionAmt\", \"card1\", \"card2\"])\n",
    "identity_df = load_csv_efficiently(os.path.join(DATA_DIR, \"train_identity.csv\"), usecols=[\"TransactionID\", \"DeviceType\"])\n",
    "\n",
    "# ✅ Load SEC EDGAR Financial Data (NEW)\n",
    "sec_num_df = load_csv_efficiently(os.path.join(DATA_DIR, \"sec_num_data.csv\"), usecols=[\"adsh\", \"value\"])\n",
    "sec_sub_df = load_csv_efficiently(os.path.join(DATA_DIR, \"sec_sub_data.csv\"), usecols=[\"adsh\", \"sic\", \"form\"])\n",
    "\n",
    "print(\"✅ Datasets loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5712a432-0def-4f01-81df-977d2b98e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize memory\n",
    "def optimize_memory(df):\n",
    "    \"\"\"Converts data types to reduce memory usage.\"\"\"\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = df[col].astype(np.int32)\n",
    "    return df\n",
    "\n",
    "credit_card_df = optimize_memory(credit_card_df)\n",
    "paysim_df = optimize_memory(paysim_df)\n",
    "transaction_df = optimize_memory(transaction_df)\n",
    "identity_df = optimize_memory(identity_df)\n",
    "sec_num_df = optimize_memory(sec_num_df)\n",
    "sec_sub_df = optimize_memory(sec_sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "034b1575-5948-49ed-b52d-86b4f55d9497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and process data\n",
    "credit_card_df.rename(columns={\"Class\": \"isFraud\", \"Amount\": \"TransactionAmt\"}, inplace=True)\n",
    "paysim_df.rename(columns={\"amount\": \"TransactionAmt\"}, inplace=True)\n",
    "ieee_merged_df = transaction_df.merge(identity_df, on=\"TransactionID\", how=\"left\")\n",
    "\n",
    "# ✅ Merge SEC Data on `adsh` Column\n",
    "sec_merged_df = sec_num_df.merge(sec_sub_df, on=\"adsh\", how=\"left\")\n",
    "\n",
    "# Merge all datasets\n",
    "transaction_data = pd.concat([credit_card_df, paysim_df, ieee_merged_df, sec_merged_df], axis=0, ignore_index=True)\n",
    "transaction_data.drop(columns=[\"TransactionID\", \"adsh\"], inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69f43dfd-1d25-4396-8360-142c7f03713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = transaction_data.select_dtypes(include=['number']).columns\n",
    "categorical_cols = transaction_data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "# Handle missing values for numeric columns using median\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "transaction_data[numeric_cols] = num_imputer.fit_transform(transaction_data[numeric_cols])\n",
    "\n",
    "# Handle missing values for categorical columns using most frequent value\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "transaction_data[categorical_cols] = cat_imputer.fit_transform(transaction_data[categorical_cols])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8e1c61a-2990-4734-8b28-7abd03611a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Encode categorical variables\n",
    "cat_cols = transaction_data.select_dtypes(include=['object']).columns\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    transaction_data[col] = le.fit_transform(transaction_data[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b85fe6b-b974-4b13-afe5-ac5f22ca7f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "num_cols = transaction_data.select_dtypes(include=[\"int32\", \"float32\"]).columns\n",
    "transaction_data[num_cols] = scaler.fit_transform(transaction_data[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8b692c6-5297-4645-9255-5d139b2288f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Generating synthetic fraud transactions using Variational Autoencoder (VAE)...\n",
      "✅ Generated 5000 synthetic fraud transactions!\n"
     ]
    }
   ],
   "source": [
    "# ============== 2. SYNTHETIC DATA USING VAE ==============\n",
    "print(\"🤖 Generating synthetic fraud transactions using Variational Autoencoder (VAE)...\")\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=16):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n",
    "\n",
    "fraud_data = transaction_data[transaction_data[\"isFraud\"] == 1].drop(columns=[\"isFraud\"])\n",
    "input_dim = fraud_data.shape[1]\n",
    "X_train_tensor = torch.tensor(fraud_data.values, dtype=torch.float32)\n",
    "dataset = TensorDataset(X_train_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "vae = VAE(input_dim)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed = vae(batch[0])\n",
    "        loss = loss_fn(reconstructed, batch[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "latent_space = torch.randn(SYNTHETIC_DATA_SIZE, 16)\n",
    "synthetic_data = vae.decoder(latent_space).detach().numpy()\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=fraud_data.columns)\n",
    "synthetic_df[\"isFraud\"] = 1\n",
    "\n",
    "print(f\"✅ Generated {SYNTHETIC_DATA_SIZE} synthetic fraud transactions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d0cf2a7-0735-4ba7-b38b-39109707baae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\users\\sahan\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\sahan\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\sahan\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089b216e-f7ae-4778-9bbe-68b1cd45b989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Generating additional synthetic transactions using Mistral 7B...\n",
      "✅ Generated 0 transactions using Gen AI via Hugging Face API!\n"
     ]
    }
   ],
   "source": [
    "# ============== 3. SYNTHETIC DATA USING GEN AI (Mistral 7B) ==============\n",
    "print(\"🤖 Generating additional synthetic transactions using Mistral 7B...\")\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Hugging Face API Key (replace with your key)\n",
    "HUGGINGFACE_API_KEY = \"***\"\n",
    "API_URL = \"**\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {HUGGINGFACE_API_KEY}\"}\n",
    "\n",
    "def generate_fraudulent_transaction():\n",
    "    \"\"\"Generates synthetic financial fraud transactions using Hugging Face API.\"\"\"\n",
    "    prompt = \"\"\"\n",
    "    Generate a JSON representation of a fraudulent financial transaction with realistic attributes:\n",
    "    - TransactionAmt: A realistic amount (e.g., 1000.45)\n",
    "    - Card1, Card2: Numeric values representing card IDs\n",
    "    - DeviceType: \"desktop\" or \"mobile\"\n",
    "    - isFraud: Always 1\n",
    "    \"\"\"\n",
    "\n",
    "    payload = {\"inputs\": prompt, \"parameters\": {\"max_length\": 150, \"temperature\": 0.7}}\n",
    "    response = requests.post(API_URL, headers=HEADERS, json=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            json_str = response.json()[0]['generated_text']\n",
    "            synthetic_transaction = eval(json_str)  # Convert string to dictionary\n",
    "            return synthetic_transaction\n",
    "        except:\n",
    "            return None  # Skip invalid output\n",
    "    else:\n",
    "        print(f\"❌ API Error: {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Generate 1000 synthetic transactions\n",
    "synthetic_gen_ai_data = []\n",
    "for _ in range(50):\n",
    "    transaction = generate_fraudulent_transaction()\n",
    "    if transaction:\n",
    "        synthetic_gen_ai_data.append(transaction)\n",
    "\n",
    "synthetic_gen_ai_df = pd.DataFrame(synthetic_gen_ai_data)\n",
    "\n",
    "print(f\"✅ Generated {len(synthetic_gen_ai_df)} transactions using Gen AI via Hugging Face API!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee52039d-356f-4192-87d5-c3a10b6c5924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final dataset shape: (10296973, 8)\n",
      "🎉 Final dataset saved as final_dataset.csv!\n"
     ]
    }
   ],
   "source": [
    "# ============== 4. FINAL DATASET & SAVE ==============\n",
    "final_balanced_data = pd.concat([transaction_data, synthetic_df, synthetic_gen_ai_df], ignore_index=True)\n",
    "\n",
    "print(f\"✅ Final dataset shape: {final_balanced_data.shape}\")\n",
    "final_balanced_data.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"🎉 Final dataset saved as {OUTPUT_FILE}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b023367-c2c7-434d-8044-540210fe2154",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3️⃣ Feature Engineering using Gen AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18771afd-9575-476b-862e-89abc29d64f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold, mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bca8d5-5b72-4cda-aac2-66ae00c5486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Waiting for user to upload dataset...\n",
      "⏳ Waiting... (5/300 sec)\n",
      "⏳ Waiting... (10/300 sec)\n",
      "⏳ Waiting... (15/300 sec)\n",
      "⏳ Waiting... (20/300 sec)\n",
      "⏳ Waiting... (25/300 sec)\n",
      "⏳ Waiting... (30/300 sec)\n",
      "⏳ Waiting... (35/300 sec)\n",
      "⏳ Waiting... (40/300 sec)\n",
      "⏳ Waiting... (45/300 sec)\n",
      "⏳ Waiting... (50/300 sec)\n",
      "⏳ Waiting... (55/300 sec)\n",
      "⏳ Waiting... (60/300 sec)\n",
      "⏳ Waiting... (65/300 sec)\n",
      "⏳ Waiting... (70/300 sec)\n",
      "⏳ Waiting... (75/300 sec)\n",
      "⏳ Waiting... (80/300 sec)\n",
      "⏳ Waiting... (85/300 sec)\n",
      "⏳ Waiting... (90/300 sec)\n",
      "⏳ Waiting... (95/300 sec)\n",
      "⏳ Waiting... (100/300 sec)\n",
      "⏳ Waiting... (105/300 sec)\n",
      "⏳ Waiting... (110/300 sec)\n",
      "⏳ Waiting... (115/300 sec)\n",
      "⏳ Waiting... (120/300 sec)\n",
      "⏳ Waiting... (125/300 sec)\n",
      "⏳ Waiting... (130/300 sec)\n",
      "⏳ Waiting... (135/300 sec)\n",
      "⏳ Waiting... (140/300 sec)\n",
      "⏳ Waiting... (145/300 sec)\n",
      "⏳ Waiting... (150/300 sec)\n",
      "⏳ Waiting... (155/300 sec)\n",
      "⏳ Waiting... (160/300 sec)\n",
      "⏳ Waiting... (165/300 sec)\n",
      "⏳ Waiting... (170/300 sec)\n",
      "⏳ Waiting... (175/300 sec)\n",
      "⏳ Waiting... (180/300 sec)\n",
      "⏳ Waiting... (185/300 sec)\n",
      "⏳ Waiting... (190/300 sec)\n",
      "⏳ Waiting... (195/300 sec)\n",
      "⏳ Waiting... (200/300 sec)\n",
      "✅ File 'uploaded_data.csv' detected! Loading...\n",
      "✅ Uploaded dataset loaded with shape: (100, 9)\n"
     ]
    }
   ],
   "source": [
    "# ============== CONFIG ==============\n",
    "UPLOAD_FILE = \"uploaded_data.csv\"  # User uploads this file dynamically\n",
    "OUTPUT_FILE = \"processed_data.csv\"\n",
    "HUGGING_FACE_API_URL = \"Add your url\" #replaced to push\n",
    "HEADERS = {\"Authorization\": \"Bearer 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\"}  # Replace with your Hugging Face API key\n",
    "\n",
    "\n",
    "\n",
    "# ============== 1. LOAD DATASET ==============\n",
    "def wait_for_file(file_path, timeout=300):\n",
    "    \"\"\"\n",
    "    Waits for the user to upload the CSV file. Checks every 5 seconds for a max of `timeout` seconds.\n",
    "    \"\"\"\n",
    "    print(\"🔄 Waiting for user to upload dataset...\")\n",
    "\n",
    "    elapsed_time = 0\n",
    "    while not os.path.exists(file_path):\n",
    "        if elapsed_time >= timeout:\n",
    "            print(f\"🚨 No file uploaded within {timeout} seconds. Exiting.\")\n",
    "            exit()\n",
    "        time.sleep(5)  # Check every 5 seconds\n",
    "        elapsed_time += 5\n",
    "        print(f\"⏳ Waiting... ({elapsed_time}/{timeout} sec)\")\n",
    "\n",
    "    print(f\"✅ File '{file_path}' detected! Loading...\")\n",
    "\n",
    "def load_csv(file_path):\n",
    "    \"\"\"Loads a CSV file efficiently after user uploads it.\"\"\"\n",
    "    return pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# **Wait for user to upload file**\n",
    "wait_for_file(UPLOAD_FILE)\n",
    "\n",
    "# **Load the uploaded file**\n",
    "try:\n",
    "    uploaded_df = load_csv(UPLOAD_FILE)\n",
    "    print(f\"✅ Uploaded dataset loaded with shape: {uploaded_df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading CSV: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88b8aa9c-3df4-4fc4-b32d-3440e48a7c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== 2. HANDLE MISSING VALUES ==============\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"Handles missing values: median for numerical, most-frequent for categorical.\"\"\"\n",
    "    num_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    cat_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    \n",
    "    if len(num_cols) > 0:\n",
    "        num_imputer = SimpleImputer(strategy=\"median\")\n",
    "        df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "    \n",
    "    if len(cat_cols) > 0:\n",
    "        cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
    "    \n",
    "    return df\n",
    "\n",
    "uploaded_df = handle_missing_values(uploaded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48f20b2c-9805-43e1-a2c0-def594373b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Selecting important features...\n",
      "✅ Selected 9 features.\n"
     ]
    }
   ],
   "source": [
    "# ============== 3. ENCODE CATEGORICAL FEATURES ==============\n",
    "def encode_categorical(df):\n",
    "    \"\"\"Encodes categorical variables using Label Encoding.\"\"\"\n",
    "    cat_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "    return df\n",
    "\n",
    "uploaded_df = encode_categorical(uploaded_df)\n",
    "\n",
    "# ============== 4. FEATURE SELECTION ==============\n",
    "def select_features(df):\n",
    "    \"\"\"Automatically selects best features for anomaly detection.\"\"\"\n",
    "    print(\"📊 Selecting important features...\")\n",
    "\n",
    "    # 4.1 Remove low variance features\n",
    "    selector = VarianceThreshold(threshold=0.01)\n",
    "    df_selected = selector.fit_transform(df)\n",
    "\n",
    "    # 4.2 Mutual Information for Feature Importance\n",
    "    X = df.drop(columns=[\"isFraud\"], errors=\"ignore\")\n",
    "    y = df[\"isFraud\"] if \"isFraud\" in df.columns else None\n",
    "\n",
    "    selected_features = X.columns  # ✅ Define it before the condition\n",
    "\n",
    "    if y is not None:\n",
    "        mi_scores = mutual_info_classif(X, y, discrete_features=\"auto\")\n",
    "        feature_scores = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\n",
    "        selected_features = feature_scores.index[:10]  # Take top 10 features\n",
    "\n",
    "    df_selected = df[selected_features]  # ✅ Always defined\n",
    "\n",
    "    print(f\"✅ Selected {df_selected.shape[1]} features.\")\n",
    "    return df_selected\n",
    "\n",
    "selected_df = select_features(uploaded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09a0104e-7a35-4002-94f9-432d81f70d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Using Hugging Face API to analyze feature importance...\n",
      "🤖 Gen AI suggests using these features: ['As of Date', 'Company', 'Account', 'AU', 'Currency', 'Primary Account', 'Secondary Account', 'GL Balance', 'iHub Balance']\n"
     ]
    }
   ],
   "source": [
    "# ============== 5. GEN AI FEATURE ANALYSIS (Hugging Face API) ==============\n",
    "print(\"🤖 Using Hugging Face API to analyze feature importance...\")\n",
    "\n",
    "def analyze_features_with_genai(feature_names):\n",
    "    \"\"\"Calls Hugging Face API to determine the best features for anomaly detection.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    The following are column names from a financial transactions dataset:\n",
    "    {', '.join(feature_names)}\n",
    "    \n",
    "    Based on your expertise, suggest the top 5 features that are most relevant for anomaly detection in financial fraud cases.\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(HUGGING_FACE_API_URL, headers=HEADERS, json={\"inputs\": prompt})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        output_text = response.json()[0][\"generated_text\"]\n",
    "        \n",
    "        # Extract features from Gen AI response\n",
    "        suggested_features = []\n",
    "        for feature in feature_names:\n",
    "            if feature in output_text:\n",
    "                suggested_features.append(feature)\n",
    "\n",
    "        print(f\"🤖 Gen AI suggests using these features: {suggested_features}\")\n",
    "        return suggested_features[:5]  # Take top 5 from Gen AI suggestion\n",
    "    else:\n",
    "        print(f\"⚠️ Error: {response.status_code} - {response.text}\")\n",
    "        return feature_names[:5]  # Default to first 5 features if API fails\n",
    "\n",
    "genai_selected_features = analyze_features_with_genai(selected_df.columns)\n",
    "selected_df = selected_df[genai_selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4963022-c331-4503-bfe8-c8cd94bc23da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 Processed dataset saved as processed_data.csv!\n"
     ]
    }
   ],
   "source": [
    "# ============== 6. FALLBACK MECHANISM ==============\n",
    "if len(genai_selected_features) == 0:\n",
    "    print(\"⚠️ Gen AI did not return any valid features, falling back to statistical selection.\")\n",
    "    selected_df = select_features(uploaded_df)  # Use original statistical feature selection\n",
    "else:\n",
    "    selected_df = selected_df[genai_selected_features]\n",
    "# ============== 6. FEATURE SCALING ==============\n",
    "scaler = StandardScaler()\n",
    "selected_df = pd.DataFrame(scaler.fit_transform(selected_df), columns=selected_df.columns)\n",
    "\n",
    "# ============== 7. SAVE FINAL DATASET ==============\n",
    "selected_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"🎉 Processed dataset saved as {OUTPUT_FILE}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27d8659d-9bc5-4f39-8728-a78fc97964d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681c727-2d53-4155-9478-9e87f8898c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading training and user-uploaded datasets...\n",
      "✅ Training dataset loaded with shape: (10296973, 8)\n",
      "✅ User dataset loaded with shape: (100, 9)\n",
      "📊 Selecting important features...\n",
      "✅ Selected 7 features.\n",
      "📊 Selecting important features...\n",
      "✅ Selected 7 features.\n",
      "🤖 Using Hugging Face API to analyze feature importance...\n",
      "🤖 Gen AI suggests using these features: ['form', 'DeviceType', 'TransactionAmt', 'value', 'card1', 'card2', 'sic']\n",
      "⚠️ User dataset is empty or missing! Skipping scaling.\n",
      "🎉 Processed training dataset saved as processed_train_data_final.csv!\n",
      "🎉 Processed user dataset saved as processed_data.csv!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold, mutual_info_classif\n",
    "import joblib\n",
    "\n",
    "# ============== CONFIG ==============\n",
    "TRAIN_FILE = \"final_dataset.csv\"  # For model training\n",
    "USER_UPLOAD_FILE = \"uploaded_data.csv\"  # User's input file for real-time detection\n",
    "PROCESSED_TRAIN_OUTPUT = \"processed_train_data_final.csv\"  # Final training data\n",
    "PROCESSED_USER_OUTPUT = \"processed_data.csv\"  # Processed user data for API\n",
    "HUGGING_FACE_API_URL = \"Add your url\" #replaced to push\n",
    "HEADERS = {\"Authorization\": \"Bearer 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\"}  # Replace with your Hugging Face API key\n",
    "\n",
    "# ============== 1. LOAD DATASET ==============\n",
    "def load_csv(file_path):\n",
    "    \"\"\"Loads a CSV file dynamically, waits for user upload if missing.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"🚨 No file found! Please upload '{file_path}' first.\")\n",
    "    return pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "print(\"🔄 Loading training and user-uploaded datasets...\")\n",
    "try:\n",
    "    train_df = load_csv(TRAIN_FILE)  # Training dataset\n",
    "    print(f\"✅ Training dataset loaded with shape: {train_df.shape}\")\n",
    "    \n",
    "    if os.path.exists(USER_UPLOAD_FILE):\n",
    "        user_df = load_csv(USER_UPLOAD_FILE)  # User-uploaded dataset\n",
    "        print(f\"✅ User dataset loaded with shape: {user_df.shape}\")\n",
    "    else:\n",
    "        user_df = None  # No user upload yet\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "# ============== 2. HANDLE MISSING VALUES ==============\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"Handles missing values: median for numerical, most-frequent for categorical.\"\"\"\n",
    "    num_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    cat_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    \n",
    "    if len(num_cols) > 0:\n",
    "        num_imputer = SimpleImputer(strategy=\"median\")\n",
    "        df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "    \n",
    "    if len(cat_cols) > 0:\n",
    "        cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = handle_missing_values(train_df)\n",
    "if user_df is not None:\n",
    "    user_df = handle_missing_values(user_df)\n",
    "\n",
    "# ============== 3. ENCODE CATEGORICAL FEATURES ==============\n",
    "def encode_categorical(df):\n",
    "    \"\"\"Encodes categorical variables using Label Encoding.\"\"\"\n",
    "    cat_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "    return df\n",
    "\n",
    "train_df = encode_categorical(train_df)\n",
    "if user_df is not None:\n",
    "    user_df = encode_categorical(user_df)\n",
    "\n",
    "# ============== 4. FEATURE SELECTION ==============\n",
    "def select_features(df):\n",
    "    \"\"\"Automatically selects best features for anomaly detection, ensuring non-empty data.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"⚠️ Dataset is empty! Skipping feature selection.\")\n",
    "        return df  # Return unchanged\n",
    "\n",
    "    print(\"📊 Selecting important features...\")\n",
    "\n",
    "    # Store original column names\n",
    "    original_columns = df.columns\n",
    "\n",
    "    # Remove low variance features\n",
    "    selector = VarianceThreshold(threshold=0.01)\n",
    "    df_selected = selector.fit_transform(df)\n",
    "\n",
    "    # Restore column names for selected features\n",
    "    selected_mask = selector.get_support()  # Get True/False mask of selected features\n",
    "    selected_columns = original_columns[selected_mask]  # Apply mask to get column names\n",
    "\n",
    "    df_selected = pd.DataFrame(df_selected, columns=selected_columns)  # Convert back to DataFrame\n",
    "\n",
    "    # Mutual Information for Feature Importance (Only if \"isFraud\" column exists)\n",
    "    if \"isFraud\" in df.columns:\n",
    "        X = df_selected\n",
    "        y = df[\"isFraud\"]\n",
    "        \n",
    "        mi_scores = mutual_info_classif(X, y, discrete_features=\"auto\")\n",
    "        feature_scores = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\n",
    "        top_features = feature_scores.index[:10]  # Take top 10 features\n",
    "        df_selected = df_selected[top_features]\n",
    "\n",
    "    print(f\"✅ Selected {df_selected.shape[1]} features.\")\n",
    "    return df_selected\n",
    "\n",
    "\n",
    "train_df = select_features(train_df)\n",
    "if user_df is not None:\n",
    "    user_df = select_features(user_df)\n",
    "\n",
    "# ============== 5. GEN AI FEATURE ANALYSIS (Hugging Face API) ==============\n",
    "print(\"🤖 Using Hugging Face API to analyze feature importance...\")\n",
    "\n",
    "def analyze_features_with_genai(feature_names):\n",
    "    \"\"\"Calls Hugging Face API to determine the best features for anomaly detection.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    The following are column names from a financial transactions dataset:\n",
    "    {', '.join(feature_names)}\n",
    "    \n",
    "    Based on your expertise, suggest the top 5 features that are most relevant for anomaly detection in financial fraud cases.\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(HUGGING_FACE_API_URL, headers=HEADERS, json={\"inputs\": prompt})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        output_text = response.json()[0][\"generated_text\"]\n",
    "        \n",
    "        # Extract features from Gen AI response\n",
    "        suggested_features = []\n",
    "        for feature in feature_names:\n",
    "            if feature in output_text:\n",
    "                suggested_features.append(feature)\n",
    "\n",
    "        print(f\"🤖 Gen AI suggests using these features: {suggested_features}\")\n",
    "        return suggested_features[:5]  # Take top 5 from Gen AI suggestion\n",
    "    else:\n",
    "        print(f\"⚠️ Error: {response.status_code} - {response.text}\")\n",
    "        return feature_names[:5]  # Default to first 5 features if API fails\n",
    "\n",
    "genai_selected_features = analyze_features_with_genai(train_df.columns)\n",
    "\n",
    "\n",
    "# ✅ Handle missing columns more robustly\n",
    "available_features = [f for f in genai_selected_features if f in train_df.columns]\n",
    "\n",
    "if not available_features:\n",
    "    print(\"⚠️ No Gen AI suggested features found in dataset. Using first 5 available features instead.\")\n",
    "    available_features = train_df.columns[:5].tolist()  # Convert to list to avoid indexing errors\n",
    "\n",
    "# Ensure selected columns exist before applying selection\n",
    "available_features = [f for f in available_features if f in train_df.columns]\n",
    "\n",
    "if not available_features:\n",
    "    raise ValueError(\"🚨 Critical Error: No valid features found for training. Please check dataset integrity!\")\n",
    "\n",
    "train_df = train_df[available_features]\n",
    "\n",
    "if user_df is not None:\n",
    "    user_df = user_df[[f for f in available_features if f in user_df.columns]]\n",
    "\n",
    "\n",
    "# Ensure dataset is not empty before scaling\n",
    "if not train_df.empty:\n",
    "    scaler = StandardScaler()\n",
    "    train_df = pd.DataFrame(scaler.fit_transform(train_df), columns=train_df.columns)\n",
    "    joblib.dump(scaler, \"scaler.pkl\")  # Save scaler for later use\n",
    "else:\n",
    "    print(\"⚠️ Train dataset is empty! Skipping scaling.\")\n",
    "\n",
    "if user_df is not None and not user_df.empty:\n",
    "    user_df = pd.DataFrame(scaler.transform(user_df), columns=user_df.columns)\n",
    "else:\n",
    "    print(\"⚠️ User dataset is empty or missing! Skipping scaling.\")\n",
    "    \n",
    "\n",
    "\n",
    "# ============== 7. SAVE FINAL DATASET ==============\n",
    "train_df.to_csv(PROCESSED_TRAIN_OUTPUT, index=False)\n",
    "print(f\"🎉 Processed training dataset saved as {PROCESSED_TRAIN_OUTPUT}!\")\n",
    "\n",
    "if user_df is not None:\n",
    "    user_df.to_csv(PROCESSED_USER_OUTPUT, index=False)\n",
    "    print(f\"🎉 Processed user dataset saved as {PROCESSED_USER_OUTPUT}!\")\n",
    "else:\n",
    "    print(\"📢 No user dataset found, waiting for upload.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e037b-e87d-4635-bb4c-2abe54b9606d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading processed dataset...\n",
      "✅ Loaded dataset with shape: (10296973, 5)\n",
      "🔧 Training Autoencoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahan\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m485s\u001b[0m 2ms/step - loss: 0.7439 - val_loss: 0.6339\n",
      "Epoch 2/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 2ms/step - loss: 0.8971 - val_loss: 0.6339\n",
      "Epoch 3/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 2ms/step - loss: 0.9368 - val_loss: 0.6339\n",
      "Epoch 4/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m526s\u001b[0m 2ms/step - loss: 0.8903 - val_loss: 0.6339\n",
      "Epoch 5/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m515s\u001b[0m 2ms/step - loss: 1.0166 - val_loss: 0.6339\n",
      "Epoch 6/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m510s\u001b[0m 2ms/step - loss: 0.8584 - val_loss: 0.6339\n",
      "Epoch 7/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 2ms/step - loss: 0.9459 - val_loss: 0.6339\n",
      "Epoch 8/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m513s\u001b[0m 2ms/step - loss: 0.8273 - val_loss: 0.6339\n",
      "Epoch 9/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m554s\u001b[0m 2ms/step - loss: 0.8081 - val_loss: 0.6339\n",
      "Epoch 10/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 2ms/step - loss: 0.8595 - val_loss: 0.6339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Autoencoder training complete!\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 914us/step\n",
      "📊 Autoencoder Threshold: 0.7459511914998989\n",
      "🔧 Training Isolation Forest...\n",
      "🔧 Training Local Outlier Factor (LOF)...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from pyod.models.hbos import HBOS\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ============== CONFIG ==============\n",
    "PROCESSED_TRAIN_FILE = \"processed_train_data_final.csv\"\n",
    "MODEL_SAVE_PATH = \"hybrid_anomaly_model.pkl\"\n",
    "AUTOENCODER_SAVE_PATH = \"autoencoder_model.h5\"\n",
    "SCALER_PATH = \"scaler.pkl\"\n",
    "\n",
    "# ============== 1. LOAD PROCESSED DATA ==============\n",
    "def load_data(file_path):\n",
    "    \"\"\"Loads processed training dataset.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"🚨 No file found! Please run Step 3 first: '{file_path}'\")\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "print(\"🔄 Loading processed dataset...\")\n",
    "df = load_data(PROCESSED_TRAIN_FILE)\n",
    "print(f\"✅ Loaded dataset with shape: {df.shape}\")\n",
    "\n",
    "# Ensure no missing values remain\n",
    "df = df.fillna(0)\n",
    "\n",
    "# ============== 2. FEATURE SCALING ==============\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "joblib.dump(scaler, SCALER_PATH)  # Save scaler for real-time detection\n",
    "\n",
    "# ============== 3. TRAIN-TEST SPLIT ==============\n",
    "X_train, X_test = train_test_split(df_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# ============== 4. AUTOENCODER MODEL ==============\n",
    "def build_autoencoder(input_dim):\n",
    "    \"\"\"Builds and compiles an autoencoder model.\"\"\"\n",
    "    encoder = keras.Sequential([\n",
    "        keras.layers.Dense(64, activation=\"relu\", input_shape=(input_dim,)),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(16, activation=\"relu\")\n",
    "    ])\n",
    "\n",
    "    decoder = keras.Sequential([\n",
    "        keras.layers.Dense(32, activation=\"relu\", input_shape=(16,)),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(input_dim, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    autoencoder = keras.Sequential([encoder, decoder])\n",
    "    autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return autoencoder, encoder\n",
    "\n",
    "print(\"🔧 Training Autoencoder...\")\n",
    "autoencoder, encoder = build_autoencoder(X_train.shape[1])\n",
    "autoencoder.fit(X_train, X_train, epochs=10, batch_size=32, validation_data=(X_test, X_test), verbose=1)\n",
    "autoencoder.save(AUTOENCODER_SAVE_PATH)  # Save model\n",
    "print(\"✅ Autoencoder training complete!\")\n",
    "\n",
    "# Compute reconstruction errors\n",
    "train_errors = np.mean(np.abs(autoencoder.predict(X_train) - X_train), axis=1)\n",
    "threshold = np.percentile(train_errors, 95)  # 95th percentile as anomaly threshold\n",
    "print(f\"📊 Autoencoder Threshold: {threshold}\")\n",
    "\n",
    "# ============== 5. TRAIN OTHER ANOMALY DETECTION MODELS ==============\n",
    "print(\"🔧 Training Isolation Forest...\")\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "iso_forest.fit(X_train)\n",
    "\n",
    "print(\"🔧 Training Local Outlier Factor (LOF)...\")\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "lof.fit(X_train)\n",
    "\n",
    "print(\"🔧 Training HBOS...\")\n",
    "hbos = HBOS(contamination=0.05)\n",
    "hbos.fit(X_train)\n",
    "\n",
    "# ============== 6. HYBRID MODEL PREDICTION ==============\n",
    "def predict_anomalies(X):\n",
    "    \"\"\"Runs data through all models and combines results.\"\"\"\n",
    "    autoencoder_preds = np.mean(np.abs(autoencoder.predict(X) - X), axis=1) > threshold\n",
    "    iso_preds = iso_forest.predict(X) == -1  # -1 means anomaly\n",
    "    lof_preds = lof.fit_predict(X) == -1  # -1 means anomaly\n",
    "    hbos_preds = hbos.predict(X) == 1  # 1 means anomaly in HBOS\n",
    "\n",
    "    # Majority Voting\n",
    "    hybrid_preds = (autoencoder_preds.astype(int) + iso_preds.astype(int) +\n",
    "                    lof_preds.astype(int) + hbos_preds.astype(int)) >= 2  # Majority vote\n",
    "    return hybrid_preds\n",
    "\n",
    "# ============== 7. EVALUATION METRICS ==============\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"Calculates evaluation metrics.\"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=1)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=1)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=1)\n",
    "    print(f\"📊 Model Performance: Accuracy={acc:.2f}, Precision={precision:.2f}, Recall={recall:.2f}, F1={f1:.2f}\")\n",
    "\n",
    "# Since we don't have labels in anomaly detection, assume top 5% highest reconstruction errors are anomalies\n",
    "y_test_true = (np.mean(np.abs(autoencoder.predict(X_test) - X_test), axis=1) > threshold).astype(int)\n",
    "y_test_pred = predict_anomalies(X_test)\n",
    "\n",
    "evaluate_model(y_test_true, y_test_pred)\n",
    "\n",
    "# ============== 8. SAVE MODELS ==============\n",
    "joblib.dump({\"iso_forest\": iso_forest, \"lof\": lof, \"hbos\": hbos, \"threshold\": threshold}, MODEL_SAVE_PATH)\n",
    "print(f\"✅ Hybrid Model saved to {MODEL_SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ef4c62-431b-4f79-8281-dd729239a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import requests\n",
    "import json\n",
    "from tensorflow import keras\n",
    "\n",
    "# ============== CONFIG ==============\n",
    "PROCESSED_USER_FILE = \"processed_data.csv\"  # User-uploaded data after Step 3 processing\n",
    "MODEL_PATH = \"hybrid_anomaly_model.pkl\"  # Trained hybrid model\n",
    "AUTOENCODER_PATH = \"autoencoder_model.h5\"  # Trained autoencoder model\n",
    "SCALER_PATH = \"scaler.pkl\"  # Feature scaler\n",
    "HUGGING_FACE_API_URL = \"Add your url\" #replaced to push\n",
    "HEADERS = {\"Authorization\": \"Bearer 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\"}  # Replace with your Hugging Face API key\n",
    "\n",
    "# ============== 1. LOAD MODELS & DATA ==============\n",
    "print(\"🔄 Loading trained models and user-uploaded dataset...\")\n",
    "\n",
    "# Load processed user-uploaded data\n",
    "def load_csv(file_path):\n",
    "    \"\"\"Loads a CSV file dynamically.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"🚨 No file found! Please upload '{file_path}' first.\")\n",
    "    return pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "user_df = load_csv(PROCESSED_USER_FILE)\n",
    "scaler = joblib.load(SCALER_PATH)  # Load scaler\n",
    "\n",
    "# Load trained hybrid model components\n",
    "model_components = joblib.load(MODEL_PATH)\n",
    "iso_forest = model_components[\"iso_forest\"]\n",
    "lof = model_components[\"lof\"]\n",
    "hbos = model_components[\"hbos\"]\n",
    "threshold = model_components[\"threshold\"]\n",
    "\n",
    "# Load trained Autoencoder model\n",
    "autoencoder = keras.models.load_model(AUTOENCODER_PATH)\n",
    "\n",
    "# Scale user data\n",
    "user_df_scaled = scaler.transform(user_df)\n",
    "\n",
    "# ============== 2. PREDICT ANOMALIES ==============\n",
    "print(\"🔍 Detecting anomalies...\")\n",
    "\n",
    "def predict_anomalies(X):\n",
    "    \"\"\"Runs data through all models and combines results.\"\"\"\n",
    "    autoencoder_preds = np.mean(np.abs(autoencoder.predict(X) - X), axis=1) > threshold\n",
    "    iso_preds = iso_forest.predict(X) == -1\n",
    "    lof_preds = lof.fit_predict(X) == -1\n",
    "    hbos_preds = hbos.predict(X) == 1  # 1 means anomaly in HBOS\n",
    "\n",
    "    # Majority Voting\n",
    "    hybrid_preds = (autoencoder_preds.astype(int) + iso_preds.astype(int) +\n",
    "                    lof_preds.astype(int) + hbos_preds.astype(int)) >= 2  # Majority vote\n",
    "    return hybrid_preds\n",
    "\n",
    "user_df[\"Anomaly\"] = predict_anomalies(user_df_scaled)\n",
    "anomalies = user_df[user_df[\"Anomaly\"] == 1]  # Filter detected anomalies\n",
    "\n",
    "print(f\"📢 Detected {len(anomalies)} anomalies!\")\n",
    "\n",
    "# ============== 3. ROOT CAUSE ANALYSIS USING GEN AI ==============\n",
    "def get_root_cause_analysis(row):\n",
    "    \"\"\"Calls Hugging Face API to determine the root cause of an anomaly.\"\"\"\n",
    "    input_text = f\"\"\"\n",
    "    A financial transaction was flagged as an anomaly. Here are the details:\n",
    "    {json.dumps(row.to_dict(), indent=2)}\n",
    "    \n",
    "    Can you analyze the possible root cause of this anomaly based on common fraud detection patterns?\n",
    "    \"\"\"\n",
    "    response = requests.post(HUGGING_FACE_API_URL, headers=HEADERS, json={\"inputs\": input_text})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()[0][\"generated_text\"]\n",
    "    else:\n",
    "        return \"⚠️ Root Cause Analysis unavailable due to API error.\"\n",
    "\n",
    "if len(anomalies) > 0:\n",
    "    print(\"🤖 Performing Root Cause Analysis on anomalies...\")\n",
    "    anomalies[\"Root_Cause\"] = anomalies.apply(get_root_cause_analysis, axis=1)\n",
    "\n",
    "# ============== 4. AUTOMATED CORRECTIVE ACTIONS USING GEN AI ==============\n",
    "def get_corrective_action(row):\n",
    "    \"\"\"Suggests automated corrective actions based on anomaly details.\"\"\"\n",
    "    input_text = f\"\"\"\n",
    "    A transaction was detected as an anomaly with these details:\n",
    "    {json.dumps(row.to_dict(), indent=2)}\n",
    "    \n",
    "    Based on AI-driven insights, what are the best corrective actions that can be taken?\n",
    "    \"\"\"\n",
    "    response = requests.post(HUGGING_FACE_API_URL, headers=HEADERS, json={\"inputs\": input_text})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()[0][\"generated_text\"]\n",
    "    else:\n",
    "        return \"⚠️ Corrective Actions unavailable due to API error.\"\n",
    "\n",
    "if len(anomalies) > 0:\n",
    "    print(\"🤖 Generating corrective actions for anomalies...\")\n",
    "    anomalies[\"Corrective_Action\"] = anomalies.apply(get_corrective_action, axis=1)\n",
    "\n",
    "# ============== 5. SAVE OUTPUT ==============\n",
    "anomalies.to_csv(\"anomalies_with_analysis.csv\", index=False)\n",
    "print(f\"🎉 Anomaly report saved as 'anomalies_with_analysis.csv'!\")\n",
    "\n",
    "# Display example anomaly\n",
    "if len(anomalies) > 0:\n",
    "    print(\"\\n📢 Sample Anomaly Report:\")\n",
    "    print(anomalies.head(1).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f59fc3bd-9163-420b-b5b3-2c31014377a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyod\n",
      "  Downloading pyod-2.0.4.tar.gz (169 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: joblib in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from pyod) (1.4.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from pyod) (3.9.2)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from pyod) (1.26.4)\n",
      "Requirement already satisfied: numba>=0.51 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from pyod) (0.60.0)\n",
      "Requirement already satisfied: scipy>=1.5.1 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from pyod) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from pyod) (1.5.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from numba>=0.51->pyod) (0.43.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.0->pyod) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from matplotlib->pyod) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from matplotlib->pyod) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from matplotlib->pyod) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from matplotlib->pyod) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from matplotlib->pyod) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from matplotlib->pyod) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from matplotlib->pyod) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from matplotlib->pyod) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sahan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->pyod) (1.16.0)\n",
      "Building wheels for collected packages: pyod\n",
      "  Building wheel for pyod (setup.py): started\n",
      "  Building wheel for pyod (setup.py): finished with status 'done'\n",
      "  Created wheel for pyod: filename=pyod-2.0.4-py3-none-any.whl size=200540 sha256=94c00f5c18a4387dad9ce64a577d021939fa00aff44b5c4907dc7108f81ed47c\n",
      "  Stored in directory: c:\\users\\sahan\\appdata\\local\\pip\\cache\\wheels\\c1\\f3\\c3\\67f847c010f2e3bb0515531e8f6ad3735eb1518c0f08165447\n",
      "Successfully built pyod\n",
      "Installing collected packages: pyod\n",
      "Successfully installed pyod-2.0.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2604f30f-3d79-4898-90cd-53be1662e42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m160891/160891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 2ms/step - loss: 0.0795\n",
      "Epoch 2/10\n",
      "\u001b[1m160891/160891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1933s\u001b[0m 12ms/step - loss: 0.2416\n",
      "Epoch 3/10\n",
      "\u001b[1m160891/160891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2ms/step - loss: 0.0839\n",
      "Epoch 4/10\n",
      "\u001b[1m160891/160891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 2ms/step - loss: 0.2268\n",
      "Epoch 5/10\n",
      "\u001b[1m160891/160891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 2ms/step - loss: 0.2084\n",
      "Epoch 6/10\n",
      "\u001b[1m160891/160891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 2ms/step - loss: 0.1705\n",
      "Epoch 7/10\n",
      "\u001b[1m160891/160891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 2ms/step - loss: 0.2518\n",
      "Epoch 8/10\n",
      "\u001b[1m160891/160891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 2ms/step - loss: 0.2196\n",
      "Epoch 9/10\n",
      "\u001b[1m160891/160891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 2ms/step - loss: 0.0648\n",
      "Epoch 10/10\n",
      "\u001b[1m160891/160891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 2ms/step - loss: 0.0507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m321781/321781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 887us/step\n",
      "🎉 Hybrid anomaly detection model trained and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pyod.models.hbos import HBOS\n",
    "import joblib\n",
    "\n",
    "# ============== 1. LOAD TRAINING DATASET (Step 1 & Step 2 Output) ==============\n",
    "TRAIN_FILE = \"final_dataset.csv\"  # This should contain historical + synthetic fraud data\n",
    "df = pd.read_csv(TRAIN_FILE)\n",
    "\n",
    "# ============== 2. PREPARE FEATURES ==============\n",
    "X = df.drop(columns=[\"isFraud\"], errors=\"ignore\")  # Drop target variable if it exists\n",
    "y = df[\"isFraud\"] if \"isFraud\" in df.columns else None  # Target variable\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Save the scaler for later use\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "# ============== 3. AUTOENCODER TRAINING ==============\n",
    "def build_autoencoder(input_dim):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(32, activation=\"relu\")(input_layer)\n",
    "    encoded = Dense(16, activation=\"relu\")(encoded)\n",
    "    encoded = Dense(8, activation=\"relu\")(encoded)\n",
    "    \n",
    "    decoded = Dense(16, activation=\"relu\")(encoded)\n",
    "    decoded = Dense(32, activation=\"relu\")(decoded)\n",
    "    decoded = Dense(input_dim, activation=\"linear\")(decoded)\n",
    "    \n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "autoencoder = build_autoencoder(X_scaled.shape[1])\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=10, batch_size=64, shuffle=True, verbose=1)\n",
    "\n",
    "# Save the trained autoencoder\n",
    "autoencoder.save(\"autoencoder_model.h5\")\n",
    "\n",
    "# Get reconstruction errors\n",
    "X_reconstructed = autoencoder.predict(X_scaled)\n",
    "reconstruction_error = np.mean(np.abs(X_scaled - X_reconstructed), axis=1)\n",
    "\n",
    "# ============== 4. TRAIN ISOLATION FOREST ==============\n",
    "isolation_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
    "isolation_forest.fit(X_scaled)\n",
    "iso_preds = isolation_forest.predict(X_scaled)\n",
    "\n",
    "# Convert predictions to 0 (normal) & 1 (anomaly)\n",
    "iso_preds = np.where(iso_preds == -1, 1, 0)\n",
    "\n",
    "# ============== 5. TRAIN LOCAL OUTLIER FACTOR (LOF) ==============\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "lof_preds = lof.fit_predict(X_scaled)\n",
    "\n",
    "# Convert predictions to 0 (normal) & 1 (anomaly)\n",
    "lof_preds = np.where(lof_preds == -1, 1, 0)\n",
    "\n",
    "# ============== 6. TRAIN HBOS (Histogram-Based Outlier Detection) ==============\n",
    "hbos = HBOS(contamination=0.05)\n",
    "hbos.fit(X_scaled)\n",
    "hbos_preds = hbos.predict(X_scaled)\n",
    "\n",
    "# Convert predictions to 0 (normal) & 1 (anomaly)\n",
    "hbos_preds = np.where(hbos_preds == 1, 1, 0)\n",
    "\n",
    "# Save models\n",
    "joblib.dump(isolation_forest, \"isolation_forest.pkl\")\n",
    "joblib.dump(lof, \"lof.pkl\")\n",
    "joblib.dump(hbos, \"hbos.pkl\")\n",
    "\n",
    "# ============== 7. COMBINE ALL MODELS INTO A HYBRID SYSTEM ==============\n",
    "df[\"Autoencoder_Error\"] = reconstruction_error\n",
    "df[\"IsolationForest_Pred\"] = iso_preds\n",
    "df[\"LOF_Pred\"] = lof_preds\n",
    "df[\"HBOS_Pred\"] = hbos_preds\n",
    "\n",
    "# Majority Voting: If 2 or more models detect an anomaly → Flag as anomaly\n",
    "df[\"Hybrid_Anomaly_Label\"] = (df[[\"IsolationForest_Pred\", \"LOF_Pred\", \"HBOS_Pred\"]].sum(axis=1) >= 2).astype(int)\n",
    "\n",
    "# Save the hybrid model's predictions\n",
    "df.to_csv(\"trained_hybrid_model.csv\", index=False)\n",
    "\n",
    "print(\"🎉 Hybrid anomaly detection model trained and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "14946052-1a48-44bd-a7b2-20fa2f3c4a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved selected features for model training: ['form', 'DeviceType', 'TransactionAmt', 'value', 'card1']\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Select the final features actually used in training\n",
    "selected_features = list(df.columns)  # After preprocessing, ensure these are used\n",
    "\n",
    "# Save these selected features for inference (overwrite Step 3's `selected_features.pkl`)\n",
    "joblib.dump(selected_features, \"selected_features.pkl\")\n",
    "print(f\"✅ Saved selected features for model training: {selected_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd38bf07-be36-429e-b230-8f5b2092fa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading training dataset...\n",
      "✅ Training dataset loaded with shape: (10296973, 8)\n",
      "📊 Selecting important features...\n",
      "✅ Selected 7 features.\n",
      "🤖 Using Hugging Face API to analyze feature importance...\n",
      "🤖 Gen AI suggests using these features: ['form', 'DeviceType', 'TransactionAmt', 'value', 'card1', 'card2', 'sic']\n",
      "🎉 Processed training dataset saved as processed_train_data_final.csv!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold, mutual_info_classif\n",
    "import joblib\n",
    "\n",
    "# ============== CONFIG ==============\n",
    "TRAIN_FILE = \"final_dataset.csv\"  # Training dataset from Step 1 & 2\n",
    "PROCESSED_TRAIN_OUTPUT = \"processed_train_data_final.csv\"  # Final processed training data\n",
    "FEATURES_FILE = \"selected_features.pkl\"  # Save selected features for consistency\n",
    "SCALER_FILE = \"scaler.pkl\"  # Save scaler for real-time use\n",
    "HUGGING_FACE_API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "HEADERS = {\"Authorization\": \"Bearer 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\"}  # Replace with your Hugging Face API key\n",
    "\n",
    "# ============== 1. LOAD TRAINING DATASET ==============\n",
    "def load_csv(file_path):\n",
    "    \"\"\"Loads a CSV file dynamically, waits for user upload if missing.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"🚨 No file found! Please upload '{file_path}' first.\")\n",
    "    return pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "print(\"🔄 Loading training dataset...\")\n",
    "try:\n",
    "    train_df = load_csv(TRAIN_FILE)  # Training dataset\n",
    "    print(f\"✅ Training dataset loaded with shape: {train_df.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "# ============== 2. HANDLE MISSING VALUES ==============\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"Handles missing values: median for numerical, most-frequent for categorical.\"\"\"\n",
    "    num_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    cat_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    \n",
    "    if len(num_cols) > 0:\n",
    "        num_imputer = SimpleImputer(strategy=\"median\")\n",
    "        df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "    \n",
    "    if len(cat_cols) > 0:\n",
    "        cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = handle_missing_values(train_df)\n",
    "\n",
    "# ============== 3. ENCODE CATEGORICAL FEATURES ==============\n",
    "def encode_categorical(df):\n",
    "    \"\"\"Encodes categorical variables using Label Encoding.\"\"\"\n",
    "    cat_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "    return df\n",
    "\n",
    "train_df = encode_categorical(train_df)\n",
    "\n",
    "# ============== 4. FEATURE SELECTION ==============\n",
    "def select_features(df):\n",
    "    \"\"\"Automatically selects best features for anomaly detection.\"\"\"\n",
    "    print(\"📊 Selecting important features...\")\n",
    "\n",
    "    # Remove low variance features\n",
    "    selector = VarianceThreshold(threshold=0.01)\n",
    "    df_selected = pd.DataFrame(selector.fit_transform(df), columns=df.columns[selector.get_support()])\n",
    "\n",
    "    # Mutual Information for Feature Importance\n",
    "    if \"isFraud\" in df.columns:\n",
    "        X = df.drop(columns=[\"isFraud\"])\n",
    "        y = df[\"isFraud\"]\n",
    "        mi_scores = mutual_info_classif(X, y, discrete_features=\"auto\")\n",
    "        feature_scores = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\n",
    "        selected_features = feature_scores.index[:min(10, len(feature_scores))]  # Handle cases with <10 features\n",
    "        df_selected = df[selected_features]\n",
    "    else:\n",
    "        df_selected = df  # If no labels, keep original\n",
    "\n",
    "    print(f\"✅ Selected {df_selected.shape[1]} features.\")\n",
    "    \n",
    "    # Save selected features for real-time processing consistency\n",
    "    joblib.dump(df_selected.columns.tolist(), FEATURES_FILE)\n",
    "    \n",
    "    return df_selected\n",
    "\n",
    "train_df = select_features(train_df)\n",
    "\n",
    "# ============== 5. GEN AI FEATURE ANALYSIS (Hugging Face API) ==============\n",
    "print(\"🤖 Using Hugging Face API to analyze feature importance...\")\n",
    "\n",
    "def analyze_features_with_genai(feature_names):\n",
    "    \"\"\"Calls Hugging Face API to determine the best features for anomaly detection.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    The following are column names from a financial transactions dataset:\n",
    "    {', '.join(feature_names)}\n",
    "    \n",
    "    Based on your expertise, suggest the top 5 features that are most relevant for anomaly detection in financial fraud cases.\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(HUGGING_FACE_API_URL, headers=HEADERS, json={\"inputs\": prompt})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        output_text = response.json()[0][\"generated_text\"]\n",
    "        \n",
    "        # Extract features from Gen AI response\n",
    "        suggested_features = [feature for feature in feature_names if feature in output_text]\n",
    "\n",
    "        print(f\"🤖 Gen AI suggests using these features: {suggested_features}\")\n",
    "        return suggested_features[:min(5, len(suggested_features))]  # Handle cases with <5 features\n",
    "    else:\n",
    "        print(f\"⚠️ Error: {response.status_code} - {response.text}\")\n",
    "        return feature_names[:min(5, len(feature_names))]  # Default to available features\n",
    "\n",
    "genai_selected_features = analyze_features_with_genai(train_df.columns)\n",
    "\n",
    "# ✅ **Fix: Handle missing columns dynamically**\n",
    "available_features = [f for f in genai_selected_features if f in train_df.columns]\n",
    "\n",
    "if not available_features:\n",
    "    print(\"⚠️ No Gen AI suggested features found in dataset. Using first available features instead.\")\n",
    "    available_features = train_df.columns[:min(5, len(train_df.columns))]\n",
    "\n",
    "train_df = train_df[available_features]\n",
    "\n",
    "# ============== 6. FEATURE SCALING ==============\n",
    "scaler = StandardScaler()\n",
    "train_df = pd.DataFrame(scaler.fit_transform(train_df), columns=train_df.columns)\n",
    "\n",
    "# Save the scaler for real-time processing\n",
    "joblib.dump(scaler, SCALER_FILE)\n",
    "\n",
    "# ============== 7. SAVE FINAL DATASET ==============\n",
    "train_df.to_csv(PROCESSED_TRAIN_OUTPUT, index=False)\n",
    "print(f\"🎉 Processed training dataset saved as {PROCESSED_TRAIN_OUTPUT}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c7d512a1-3cc7-4e56-9c4a-f42547d2136f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading processed dataset...\n",
      "✅ Loaded dataset with shape: (10296973, 5)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from pyod.models.hbos import HBOS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# ============== CONFIG ==============\n",
    "PROCESSED_TRAIN_FILE = \"processed_train_data_final.csv\"\n",
    "MODEL_SAVE_PATH = \"hybrid_anomaly_model.pkl\"\n",
    "AUTOENCODER_SAVE_PATH = \"autoencoder_model.h5\"\n",
    "SCALER_PATH = \"scaler.pkl\"\n",
    "\n",
    "# ============== 1. LOAD PROCESSED DATA ==============\n",
    "def load_data(file_path):\n",
    "    \"\"\"Loads processed training dataset.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"🚨 No file found! Please run Step 3 first: '{file_path}'\")\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "print(\"🔄 Loading processed dataset...\")\n",
    "df = load_data(PROCESSED_TRAIN_FILE)\n",
    "print(f\"✅ Loaded dataset with shape: {df.shape}\")\n",
    "\n",
    "# Ensure no missing values remain\n",
    "df = df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99faaa90-4126-4509-9d3e-9a444ac16ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature Scaling Complete! Scaler Saved.\n"
     ]
    }
   ],
   "source": [
    "# ============== 2. FEATURE SCALING ==============\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Save the scaler for real-time detection\n",
    "joblib.dump(scaler, SCALER_PATH)\n",
    "print(\"✅ Feature Scaling Complete! Scaler Saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47d6e2d6-8a6f-4903-bca2-ad00c5d25267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data split complete! Train Shape: (8237578, 5), Test Shape: (2059395, 5)\n",
      "🔧 Training Autoencoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahan\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m476s\u001b[0m 2ms/step - loss: 0.7526 - val_loss: 0.6339\n",
      "Epoch 2/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m471s\u001b[0m 2ms/step - loss: 0.8282 - val_loss: 0.6339\n",
      "Epoch 3/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 2ms/step - loss: 0.9895 - val_loss: 0.6339\n",
      "Epoch 4/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 2ms/step - loss: 0.8695 - val_loss: 0.6353\n",
      "Epoch 5/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m517s\u001b[0m 2ms/step - loss: 0.6653 - val_loss: 0.6339\n",
      "Epoch 6/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 2ms/step - loss: 0.9454 - val_loss: 0.6339\n",
      "Epoch 7/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 2ms/step - loss: 0.8789 - val_loss: 0.6339\n",
      "Epoch 8/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 2ms/step - loss: 0.9974 - val_loss: 0.6339\n",
      "Epoch 9/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m450s\u001b[0m 2ms/step - loss: 0.8877 - val_loss: 0.6339\n",
      "Epoch 10/10\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m507s\u001b[0m 2ms/step - loss: 0.8060 - val_loss: 0.6436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Autoencoder training complete!\n",
      "\u001b[1m257425/257425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 793us/step\n",
      "📊 Autoencoder Threshold: 0.7459511914998989\n",
      "🔧 Training Isolation Forest...\n",
      "🔧 Training Local Outlier Factor (LOF)...\n",
      "🔧 Training HBOS...\n",
      "✅ Model Training Complete!\n",
      "\u001b[1m64357/64357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 722us/step\n",
      "\u001b[1m64357/64357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 741us/step\n",
      "📊 Model Performance: Accuracy=0.99, Precision=0.92, Recall=0.81, F1=0.86\n",
      "✅ Hybrid Model saved to hybrid_anomaly_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# ============== 3. TRAIN-TEST SPLIT ==============\n",
    "X_train, X_test = train_test_split(df_scaled, test_size=0.2, random_state=42)\n",
    "print(f\"✅ Data split complete! Train Shape: {X_train.shape}, Test Shape: {X_test.shape}\")\n",
    "# ============== 4. AUTOENCODER MODEL ==============\n",
    "def build_autoencoder(input_dim):\n",
    "    \"\"\"Builds and compiles an autoencoder model.\"\"\"\n",
    "    encoder = keras.Sequential([\n",
    "        keras.layers.Dense(64, activation=\"relu\", input_shape=(input_dim,)),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(16, activation=\"relu\")\n",
    "    ])\n",
    "\n",
    "    decoder = keras.Sequential([\n",
    "        keras.layers.Dense(32, activation=\"relu\", input_shape=(16,)),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(input_dim, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    autoencoder = keras.Sequential([encoder, decoder])\n",
    "    autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return autoencoder, encoder\n",
    "\n",
    "print(\"🔧 Training Autoencoder...\")\n",
    "autoencoder, encoder = build_autoencoder(X_train.shape[1])\n",
    "\n",
    "# Train Autoencoder\n",
    "autoencoder.fit(X_train, X_train, epochs=10, batch_size=32, validation_data=(X_test, X_test), verbose=1)\n",
    "\n",
    "# Save Model\n",
    "autoencoder.save(AUTOENCODER_SAVE_PATH)\n",
    "print(\"✅ Autoencoder training complete!\")\n",
    "\n",
    "# Compute reconstruction errors\n",
    "train_errors = np.mean(np.abs(autoencoder.predict(X_train) - X_train), axis=1)\n",
    "threshold = np.percentile(train_errors, 95)  # 95th percentile as anomaly threshold\n",
    "print(f\"📊 Autoencoder Threshold: {threshold}\")\n",
    "\n",
    "# ============== 5. TRAIN OTHER MODELS ==============\n",
    "\n",
    "# Optimize Isolation Forest & LOF using subsampling for speed\n",
    "subsample_size = min(50000, len(X_train))  # Use max 50K samples for training (speeds up process)\n",
    "X_train_sampled = X_train[np.random.choice(len(X_train), subsample_size, replace=False)]\n",
    "\n",
    "print(\"🔧 Training Isolation Forest...\")\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42, n_jobs=-1)\n",
    "iso_forest.fit(X_train_sampled)\n",
    "\n",
    "print(\"🔧 Training Local Outlier Factor (LOF)...\")\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05, n_jobs=-1)\n",
    "lof.fit(X_train_sampled)\n",
    "\n",
    "print(\"🔧 Training HBOS...\")\n",
    "hbos = HBOS(contamination=0.05)\n",
    "hbos.fit(X_train_sampled)\n",
    "\n",
    "print(\"✅ Model Training Complete!\")\n",
    "# ============== 6. HYBRID MODEL PREDICTION ==============\n",
    "def predict_anomalies(X):\n",
    "    \"\"\"Runs data through all models and combines results.\"\"\"\n",
    "    autoencoder_preds = np.mean(np.abs(autoencoder.predict(X) - X), axis=1) > threshold\n",
    "    iso_preds = iso_forest.predict(X) == -1  # -1 means anomaly\n",
    "    lof_preds = lof.fit_predict(X) == -1  # -1 means anomaly\n",
    "    hbos_preds = hbos.predict(X) == 1  # 1 means anomaly in HBOS\n",
    "\n",
    "    # Majority Voting\n",
    "    hybrid_preds = (autoencoder_preds.astype(int) + iso_preds.astype(int) +\n",
    "                    lof_preds.astype(int) + hbos_preds.astype(int)) >= 2  # Majority vote\n",
    "    return hybrid_preds\n",
    "# ============== 7. EVALUATION METRICS ==============\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"Calculates evaluation metrics.\"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=1)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=1)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=1)\n",
    "    print(f\"📊 Model Performance: Accuracy={acc:.2f}, Precision={precision:.2f}, Recall={recall:.2f}, F1={f1:.2f}\")\n",
    "\n",
    "# Assume top 5% highest reconstruction errors are anomalies\n",
    "y_test_true = (np.mean(np.abs(autoencoder.predict(X_test) - X_test), axis=1) > threshold).astype(int)\n",
    "y_test_pred = predict_anomalies(X_test)\n",
    "\n",
    "evaluate_model(y_test_true, y_test_pred)\n",
    "# ============== 8. SAVE MODELS ==============\n",
    "joblib.dump({\n",
    "    \"iso_forest\": iso_forest,\n",
    "    \"lof\": lof,\n",
    "    \"hbos\": hbos,\n",
    "    \"threshold\": threshold\n",
    "}, MODEL_SAVE_PATH)\n",
    "\n",
    "print(f\"✅ Hybrid Model saved to {MODEL_SAVE_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c9b88aa3-8c21-4e42-992c-3a551706c91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved selected features for model training: ['form', 'DeviceType', 'TransactionAmt', 'value', 'card1']\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Select the final features actually used in training\n",
    "selected_features = list(df.columns)  # After preprocessing, ensure these are used\n",
    "\n",
    "# Save these selected features for inference (overwrite Step 3's `selected_features.pkl`)\n",
    "joblib.dump(selected_features, \"selected_features.pkl\")\n",
    "print(f\"✅ Saved selected features for model training: {selected_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f09ce50-a5ca-42cd-bf82-03742f950075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9f5611bf-e783-4d9f-b806-b6d0c552de10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading trained models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "C:\\Users\\sahan\\AppData\\Local\\Temp\\ipykernel_18028\\228017665.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_user[col] = 0  # Add missing column with zero values\n",
      "C:\\Users\\sahan\\AppData\\Local\\Temp\\ipykernel_18028\\228017665.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_user[col] = 0  # Add missing column with zero values\n",
      "C:\\Users\\sahan\\AppData\\Local\\Temp\\ipykernel_18028\\228017665.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_user[col] = 0  # Add missing column with zero values\n",
      "C:\\Users\\sahan\\AppData\\Local\\Temp\\ipykernel_18028\\228017665.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_user[col] = 0  # Add missing column with zero values\n",
      "C:\\Users\\sahan\\AppData\\Local\\Temp\\ipykernel_18028\\228017665.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_user[col] = 0  # Add missing column with zero values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ User dataset loaded: (100, 9)\n",
      "✅ Finalized Feature Set for Scaling: ['form', 'DeviceType', 'TransactionAmt', 'value', 'card1']\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 168ms/stepWARNING:tensorflow:5 out of the last 128723 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000018735268E00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 128723 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000018735268E00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "✅ Anomaly detection completed! Results saved to 'anomaly_results.csv'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anomaly</th>\n",
       "      <th>Root_Cause_Analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Anomaly Root_Cause_Analysis\n",
       "0    False              Normal\n",
       "1    False              Normal\n",
       "2    False              Normal\n",
       "3    False              Normal\n",
       "4    False              Normal\n",
       "5    False              Normal\n",
       "6    False              Normal\n",
       "7    False              Normal\n",
       "8    False              Normal\n",
       "9    False              Normal"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from pyod.models.hbos import HBOS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "import requests\n",
    "\n",
    "# ============== 1. LOAD TRAINED MODELS & SCALER ==============\n",
    "print(\"🔄 Loading trained models...\")\n",
    "models = joblib.load(\"hybrid_anomaly_model.pkl\")  # Load trained models\n",
    "scaler = joblib.load(\"scaler.pkl\")  # Load trained scaler\n",
    "trained_features = joblib.load(\"selected_features.pkl\")  # Load feature names used in training\n",
    "\n",
    "# Explicitly define the MSE loss function\n",
    "mse = MeanSquaredError()\n",
    "autoencoder = keras.models.load_model(\"autoencoder_model.h5\", custom_objects={\"mse\": mse})\n",
    "\n",
    "iso_forest = models[\"iso_forest\"]\n",
    "lof = models[\"lof\"]\n",
    "hbos = models[\"hbos\"]\n",
    "threshold = models[\"threshold\"]\n",
    "\n",
    "# ============== 2. LOAD USER-UPLOADED DATA ==============\n",
    "def load_user_data(file_path):\n",
    "    \"\"\"Loads user-uploaded dataset.\"\"\"\n",
    "    df_user = pd.read_csv(file_path)\n",
    "    print(f\"✅ User dataset loaded: {df_user.shape}\")\n",
    "    return df_user\n",
    "\n",
    "df_user = load_user_data(\"uploaded_data.csv\")\n",
    "\n",
    "# ============== 3. ENSURE FEATURE MATCHING ==============\n",
    "def match_features(df_user, trained_features):\n",
    "    \"\"\"\n",
    "    Ensures user-uploaded dataset has **exactly** the same columns as the trained model.\n",
    "    - Removes extra columns\n",
    "    - Adds missing columns (filled with zero)\n",
    "    - Ensures correct order\n",
    "    \"\"\"\n",
    "    user_columns = df_user.columns.tolist()\n",
    "\n",
    "    # Remove extra columns that are not in the trained model\n",
    "    df_user = df_user[[col for col in trained_features if col in user_columns]]\n",
    "\n",
    "    # Add missing columns with zero values\n",
    "    for col in trained_features:\n",
    "        if col not in df_user.columns:\n",
    "            df_user[col] = 0  # Add missing column with zero values\n",
    "\n",
    "    # Ensure correct column order\n",
    "    df_user = df_user[trained_features]\n",
    "\n",
    "    print(f\"✅ Finalized Feature Set for Scaling: {df_user.columns.tolist()}\")\n",
    "    return df_user\n",
    "\n",
    "# Apply feature matching\n",
    "df_user_selected = match_features(df_user, trained_features)\n",
    "\n",
    "# **FIX: Now the dataset has EXACTLY the same features as training**\n",
    "df_user_scaled = scaler.transform(df_user_selected)\n",
    "\n",
    "# ============== 4. PREDICT ANOMALIES USING HYBRID MODEL ==============\n",
    "def predict_anomalies(X):\n",
    "    \"\"\"Runs data through all models and combines results.\"\"\"\n",
    "    autoencoder_preds = np.mean(np.abs(autoencoder.predict(X) - X), axis=1) > threshold\n",
    "    iso_preds = iso_forest.predict(X) == -1  # -1 means anomaly\n",
    "    lof_preds = lof.fit_predict(X) == -1  # -1 means anomaly\n",
    "    hbos_preds = hbos.predict(X) == 1  # 1 means anomaly in HBOS\n",
    "\n",
    "    # Majority Voting\n",
    "    hybrid_preds = (autoencoder_preds.astype(int) + iso_preds.astype(int) +\n",
    "                    lof_preds.astype(int) + hbos_preds.astype(int)) >= 2  # Majority vote\n",
    "    return hybrid_preds\n",
    "\n",
    "df_user[\"Anomaly\"] = predict_anomalies(df_user_scaled)\n",
    "\n",
    "# ============== 5. ROOT CAUSE ANALYSIS USING GEN AI (Hugging Face API) ==============\n",
    "def analyze_root_cause(row):\n",
    "    \"\"\"Uses Hugging Face API for root cause analysis.\"\"\"\n",
    "    payload = {\"inputs\": f\"Analyze anomaly in this data: {row.to_dict()}\"}\n",
    "    response = requests.post(\"https://api-inference.huggingface.co/models/mistralai/Mistral-7B\", json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()[0][\"generated_text\"]\n",
    "    return \"No insight available\"\n",
    "\n",
    "# Apply Gen AI for insights\n",
    "df_user[\"Root_Cause_Analysis\"] = df_user.apply(lambda row: analyze_root_cause(row) if row[\"Anomaly\"] else \"Normal\", axis=1)\n",
    "\n",
    "# ============== 6. SAVE OUTPUT & DISPLAY RESULTS ==============\n",
    "df_user.to_csv(\"anomaly_results.csv\", index=False)\n",
    "print(f\"✅ Anomaly detection completed! Results saved to 'anomaly_results.csv'.\")\n",
    "df_user[[\"Anomaly\", \"Root_Cause_Analysis\"]].head(10)  # Show sample results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cb6f019c-88b9-4189-b02f-cca9c8b98ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading trained models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ User dataset loaded: (100, 9)\n",
      "✅ Finalized Feature Set: ['form', 'DeviceType', 'TransactionAmt', 'value', 'card1']\n",
      "✅ Features scaled successfully.\n",
      "✅ User dataset loaded: (100, 9)\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step \n",
      "❌ Error during Hugging Face API call: 402 Client Error: Payment Required for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\n",
      "❌ Error during Hugging Face API call: 402 Client Error: Payment Required for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\n",
      "❌ Error during Hugging Face API call: 402 Client Error: Payment Required for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\n",
      "❌ Error during Hugging Face API call: 402 Client Error: Payment Required for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\n",
      "❌ Error during Hugging Face API call: 402 Client Error: Payment Required for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\n",
      "❌ Error during Hugging Face API call: 402 Client Error: Payment Required for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\n",
      "❌ Error during Hugging Face API call: 402 Client Error: Payment Required for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\n",
      "❌ Error during Hugging Face API call: 402 Client Error: Payment Required for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\n",
      "❌ Error during Hugging Face API call: 402 Client Error: Payment Required for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\n",
      "❌ Error during Hugging Face API call: 402 Client Error: Payment Required for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\n",
      "❌ Error during Hugging Face API call: 402 Client Error: Payment Required for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\n",
      "❌ Error during Hugging Face API call: 402 Client Error: Payment Required for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\n",
      "❌ Error during Hugging Face API call: 402 Client Error: Payment Required for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\n",
      "❌ Error during Hugging Face API call: 402 Client Error: Payment Required for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\n",
      "❌ Error during Hugging Face API call: 402 Client Error: Payment Required for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\n",
      "❌ Error during Hugging Face API call: 402 Client Error: Payment Required for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\n",
      "✅ Anomaly detection completed! Results saved to 'anomaly_results.csv'.\n",
      "   Anomaly                                Root_Cause_Analysis\n",
      "0     True  Analyze anomaly in this data: {'As of Date': '...\n",
      "1     True  Analyze anomaly in this data: {'As of Date': '...\n",
      "2     True  Analyze anomaly in this data: {'As of Date': '...\n",
      "3     True  Analyze anomaly in this data: {'As of Date': '...\n",
      "4     True  Analyze anomaly in this data: {'As of Date': '...\n",
      "5     True  Analyze anomaly in this data: {'As of Date': '...\n",
      "6     True  Analyze anomaly in this data: {'As of Date': '...\n",
      "7     True  Analyze anomaly in this data: {'As of Date': '...\n",
      "8     True  Analyze anomaly in this data: {'As of Date': '...\n",
      "9     True  Analyze anomaly in this data: {'As of Date': '...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from pyod.models.hbos import HBOS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "import requests\n",
    "import os  # Import the os module\n",
    "\n",
    "# ============== 1. LOAD TRAINED MODELS & SCALER ==============\n",
    "print(\"🔄 Loading trained models...\")\n",
    "models = joblib.load(\"hybrid_anomaly_model.pkl\")  # Load trained models\n",
    "scaler = joblib.load(\"scaler.pkl\")  # Load trained scaler\n",
    "\n",
    "trained_features = ['form', 'DeviceType', 'TransactionAmt', 'value', 'card1']  # correct trained features.\n",
    "\n",
    "# Explicitly define the MSE loss function\n",
    "mse = MeanSquaredError()\n",
    "\n",
    "# Load the autoencoder with the custom loss function\n",
    "autoencoder = keras.models.load_model(\"autoencoder_model.h5\", custom_objects={\"mse\": mse})\n",
    "iso_forest = models[\"iso_forest\"]\n",
    "lof = models[\"lof\"]\n",
    "hbos = models[\"hbos\"]\n",
    "threshold = models[\"threshold\"]\n",
    "\n",
    "# ============== 2. LOAD USER-UPLOADED DATA ==============\n",
    "def load_user_data(file_path):\n",
    "    \"\"\"Loads user-uploaded dataset.\"\"\"\n",
    "    try:\n",
    "        df_user = pd.read_csv(file_path)\n",
    "        print(f\"✅ User dataset loaded: {df_user.shape}\")\n",
    "        return df_user\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: File not found at {file_path}\")\n",
    "        return None  # Return None if file not found\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "# ============== 3. FEATURE SELECTION & MAPPING ==============\n",
    "def ensure_column_compatibility(df_user, trained_features):\n",
    "    \"\"\"Ensures the user-uploaded dataset matches the trained model's feature names.\"\"\"\n",
    "    user_columns = df_user.columns.tolist()\n",
    "\n",
    "    # Semantic Mapping Rules\n",
    "    rename_mapping = {}\n",
    "    for user_col in user_columns:\n",
    "        if user_col.lower() in [\"account\", \"customer id\", \"custid\", \"account number\"]:\n",
    "            rename_mapping[user_col] = \"card1\"\n",
    "        elif user_col.lower() in [\"device\", \"device type\", \"access unit\"]:\n",
    "            rename_mapping[user_col] = \"DeviceType\"\n",
    "        elif user_col.lower() in [\"transaction amount\", \"amount\", \"gl balance\"]:\n",
    "            rename_mapping[user_col] = \"TransactionAmt\"\n",
    "        elif user_col.lower() in [\"value\", \"ihub balance\"]:\n",
    "            rename_mapping[user_col] = \"value\"\n",
    "        elif user_col.lower() in [\"form\"]:\n",
    "            rename_mapping[user_col] = \"form\"\n",
    "\n",
    "    df_user = df_user.rename(columns=rename_mapping)\n",
    "    user_columns = df_user.columns.tolist()\n",
    "\n",
    "    matching_cols = [col for col in trained_features if col in user_columns]\n",
    "\n",
    "    if not matching_cols:\n",
    "        print(\"⚠️ Renaming failed. Cannot match all required columns.\")\n",
    "        print(\"Required columns:\", trained_features)\n",
    "        print(\"Renamed columns:\", user_columns)\n",
    "        return None\n",
    "\n",
    "    df_user = df_user[matching_cols]\n",
    "\n",
    "    for col in trained_features:\n",
    "        if col not in df_user.columns:\n",
    "            df_user[col] = 0  # Neutral value for missing columns\n",
    "\n",
    "    df_user = df_user[trained_features]\n",
    "    print(f\"✅ Finalized Feature Set: {df_user.columns.tolist()}\")\n",
    "    return df_user\n",
    "\n",
    "# ============== 4. PREPROCESS NEW DATA ==============\n",
    "def preprocess_new_data(file_path, trained_features, scaler):\n",
    "    \"\"\"Preprocesses new data for anomaly detection.\"\"\"\n",
    "    df_user = load_user_data(file_path)\n",
    "    if df_user is None:\n",
    "        return None\n",
    "\n",
    "    df_user_numerical = df_user.select_dtypes(include=[np.number])\n",
    "    df_user_compatible = ensure_column_compatibility(df_user_numerical, trained_features)\n",
    "\n",
    "    if df_user_compatible is None:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df_user_scaled = scaler.transform(df_user_compatible)\n",
    "        print(\"✅ Features scaled successfully.\")\n",
    "        return df_user_scaled\n",
    "    except ValueError as e:\n",
    "        print(f\"❌ Error during scaling: {e}\")\n",
    "        print(\"Please ensure your uploaded data's columns match the trained model's required columns.\")\n",
    "        return None\n",
    "\n",
    "# ============== 5. PREDICT ANOMALIES USING HYBRID MODEL ==============\n",
    "def predict_anomalies(X):\n",
    "    \"\"\"Runs data through all models and combines results.\"\"\"\n",
    "    autoencoder_preds = np.mean(np.abs(autoencoder.predict(X) - X), axis=1) > threshold\n",
    "    iso_preds = iso_forest.predict(X) == -1  # -1 means anomaly\n",
    "    lof_preds = lof.fit_predict(X) == -1  # -1 means anomaly\n",
    "    hbos_preds = hbos.predict(X) == 1  # 1 means anomaly in HBOS\n",
    "\n",
    "    # Majority Voting\n",
    "    hybrid_preds = (autoencoder_preds.astype(int) + iso_preds.astype(int) +\n",
    "                    lof_preds.astype(int) + hbos_preds.astype(int)) >= 2  # Majority vote\n",
    "    return hybrid_preds\n",
    "\n",
    "# ============== 6. ROOT CAUSE ANALYSIS USING GEN AI (Hugging Face API) ==============\n",
    "def analyze_root_cause(row):\n",
    "    \"\"\"Uses Hugging Face API for root cause analysis.\"\"\"\n",
    "    # Get Hugging Face API token from environment variable\n",
    "    api_token = \"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\"\n",
    "\n",
    "    if not api_token:\n",
    "        print(\"❌ Error: Hugging Face API token not found in environment variables.\")\n",
    "        return \"No insight available\"\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "    payload = {\"inputs\": f\"Analyze anomaly in this data: {row.to_dict()}\"}\n",
    "    try:\n",
    "        response = requests.post(\"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\", json=payload, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        return response.json()[0][\"generated_text\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ Error during Hugging Face API call: {e}\")\n",
    "        return \"No insight available\"\n",
    "\n",
    "# ============== 7. MAIN EXECUTION ==============\n",
    "file_path = \"uploaded_data.csv\"\n",
    "scaled_data = preprocess_new_data(file_path, trained_features, scaler)\n",
    "\n",
    "if scaled_data is not None:\n",
    "    df_user = load_user_data(file_path)\n",
    "    df_user[\"Anomaly\"] = predict_anomalies(scaled_data)\n",
    "    df_user[\"Root_Cause_Analysis\"] = df_user.apply(lambda row: analyze_root_cause(row) if row[\"Anomaly\"] else \"Normal\", axis=1)\n",
    "    df_user.to_csv(\"anomaly_results.csv\", index=False)\n",
    "    print(f\"✅ Anomaly detection completed! Results saved to 'anomaly_results.csv'.\")\n",
    "    print(df_user[[\"Anomaly\", \"Root_Cause_Analysis\"]].head(10))\n",
    "else:\n",
    "    print(\"❌ Anomaly detection failed due to preprocessing errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cada5b16-c651-449f-8946-2cb56a94b8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading trained models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ User dataset loaded: (10, 9)\n",
      "✅ Finalized Feature Set: ['form', 'DeviceType', 'TransactionAmt', 'value', 'card1']\n",
      "✅ Features scaled successfully.\n",
      "✅ User dataset loaded: (10, 9)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahan\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_lof.py:283: UserWarning: n_neighbors (20) is greater than the total number of samples (10). n_neighbors will be set to (n_samples - 1) for estimation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Anomaly detection & corrective actions completed! Results saved to 'anomaly_results.csv'.\n",
      "   Anomaly                                Root_Cause_Analysis  \\\n",
      "0     True  Analyze anomaly in this data: {'As of Date': '...   \n",
      "1     True  Analyze anomaly in this data: {'As of Date': '...   \n",
      "2     True  Analyze anomaly in this data: {'As of Date': '...   \n",
      "3     True  Analyze anomaly in this data: {'As of Date': '...   \n",
      "4     True  Analyze anomaly in this data: {'As of Date': '...   \n",
      "5     True  Analyze anomaly in this data: {'As of Date': '...   \n",
      "6     True  Analyze anomaly in this data: {'As of Date': '...   \n",
      "7     True  Analyze anomaly in this data: {'As of Date': '...   \n",
      "8     True  Analyze anomaly in this data: {'As of Date': '...   \n",
      "9     True  Analyze anomaly in this data: {'As of Date': '...   \n",
      "\n",
      "                                   Corrective_Action  \n",
      "0  Suggest corrective action for this anomaly: {'...  \n",
      "1  Suggest corrective action for this anomaly: {'...  \n",
      "2  Suggest corrective action for this anomaly: {'...  \n",
      "3  Suggest corrective action for this anomaly: {'...  \n",
      "4  Suggest corrective action for this anomaly: {'...  \n",
      "5  Suggest corrective action for this anomaly: {'...  \n",
      "6  Suggest corrective action for this anomaly: {'...  \n",
      "7  Suggest corrective action for this anomaly: {'...  \n",
      "8  Suggest corrective action for this anomaly: {'...  \n",
      "9  Suggest corrective action for this anomaly: {'...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from pyod.models.hbos import HBOS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "import requests\n",
    "import os  # Import the os module\n",
    "\n",
    "# ============== 1. LOAD TRAINED MODELS & SCALER ==============\n",
    "print(\"🔄 Loading trained models...\")\n",
    "models = joblib.load(\"hybrid_anomaly_model.pkl\")  # Load trained models\n",
    "scaler = joblib.load(\"scaler.pkl\")  # Load trained scaler\n",
    "\n",
    "trained_features = ['form', 'DeviceType', 'TransactionAmt', 'value', 'card1']  # correct trained features.\n",
    "\n",
    "# Explicitly define the MSE loss function\n",
    "mse = MeanSquaredError()\n",
    "\n",
    "# Load the autoencoder with the custom loss function\n",
    "autoencoder = keras.models.load_model(\"autoencoder_model.h5\", custom_objects={\"mse\": mse})\n",
    "iso_forest = models[\"iso_forest\"]\n",
    "lof = models[\"lof\"]\n",
    "hbos = models[\"hbos\"]\n",
    "threshold = models[\"threshold\"]\n",
    "\n",
    "# ============== 2. LOAD USER-UPLOADED DATA ==============\n",
    "def load_user_data(file_path):\n",
    "    \"\"\"Loads user-uploaded dataset.\"\"\"\n",
    "    try:\n",
    "        df_user = pd.read_csv(file_path)\n",
    "        print(f\"✅ User dataset loaded: {df_user.shape}\")\n",
    "        return df_user\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: File not found at {file_path}\")\n",
    "        return None  # Return None if file not found\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "# ============== 3. FEATURE SELECTION & MAPPING ==============\n",
    "def ensure_column_compatibility(df_user, trained_features):\n",
    "    \"\"\"Ensures the user-uploaded dataset matches the trained model's feature names.\"\"\"\n",
    "    user_columns = df_user.columns.tolist()\n",
    "\n",
    "    # Semantic Mapping Rules\n",
    "    rename_mapping = {}\n",
    "    for user_col in user_columns:\n",
    "        if user_col.lower() in [\"account\", \"customer id\", \"custid\", \"account number\"]:\n",
    "            rename_mapping[user_col] = \"card1\"\n",
    "        elif user_col.lower() in [\"device\", \"device type\", \"access unit\"]:\n",
    "            rename_mapping[user_col] = \"DeviceType\"\n",
    "        elif user_col.lower() in [\"transaction amount\", \"amount\", \"gl balance\"]:\n",
    "            rename_mapping[user_col] = \"TransactionAmt\"\n",
    "        elif user_col.lower() in [\"value\", \"ihub balance\"]:\n",
    "            rename_mapping[user_col] = \"value\"\n",
    "        elif user_col.lower() in [\"form\"]:\n",
    "            rename_mapping[user_col] = \"form\"\n",
    "\n",
    "    df_user = df_user.rename(columns=rename_mapping)\n",
    "    user_columns = df_user.columns.tolist()\n",
    "\n",
    "    matching_cols = [col for col in trained_features if col in user_columns]\n",
    "\n",
    "    if not matching_cols:\n",
    "        print(\"⚠️ Renaming failed. Cannot match all required columns.\")\n",
    "        print(\"Required columns:\", trained_features)\n",
    "        print(\"Renamed columns:\", user_columns)\n",
    "        return None\n",
    "\n",
    "    df_user = df_user[matching_cols]\n",
    "\n",
    "    for col in trained_features:\n",
    "        if col not in df_user.columns:\n",
    "            df_user[col] = 0  # Neutral value for missing columns\n",
    "\n",
    "    df_user = df_user[trained_features]\n",
    "    print(f\"✅ Finalized Feature Set: {df_user.columns.tolist()}\")\n",
    "    return df_user\n",
    "\n",
    "# ============== 4. PREPROCESS NEW DATA ==============\n",
    "def preprocess_new_data(file_path, trained_features, scaler):\n",
    "    \"\"\"Preprocesses new data for anomaly detection.\"\"\"\n",
    "    df_user = load_user_data(file_path)\n",
    "    if df_user is None:\n",
    "        return None\n",
    "\n",
    "    df_user_numerical = df_user.select_dtypes(include=[np.number])\n",
    "    df_user_compatible = ensure_column_compatibility(df_user_numerical, trained_features)\n",
    "\n",
    "    if df_user_compatible is None:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df_user_scaled = scaler.transform(df_user_compatible)\n",
    "        print(\"✅ Features scaled successfully.\")\n",
    "        return df_user_scaled\n",
    "    except ValueError as e:\n",
    "        print(f\"❌ Error during scaling: {e}\")\n",
    "        print(\"Please ensure your uploaded data's columns match the trained model's required columns.\")\n",
    "        return None\n",
    "\n",
    "# ============== 5. PREDICT ANOMALIES USING HYBRID MODEL ==============\n",
    "def predict_anomalies(X):\n",
    "    \"\"\"Runs data through all models and combines results.\"\"\"\n",
    "    autoencoder_preds = np.mean(np.abs(autoencoder.predict(X) - X), axis=1) > threshold\n",
    "    iso_preds = iso_forest.predict(X) == -1  # -1 means anomaly\n",
    "    lof_preds = lof.fit_predict(X) == -1  # -1 means anomaly\n",
    "    hbos_preds = hbos.predict(X) == 1  # 1 means anomaly in HBOS\n",
    "\n",
    "    # Majority Voting\n",
    "    hybrid_preds = (autoencoder_preds.astype(int) + iso_preds.astype(int) +\n",
    "                    lof_preds.astype(int) + hbos_preds.astype(int)) >= 2  # Majority vote\n",
    "    return hybrid_preds\n",
    "\n",
    "# ============== 6. ROOT CAUSE ANALYSIS USING GEN AI (Hugging Face API) ==============\n",
    "def analyze_root_cause(row):\n",
    "    \"\"\"Uses Hugging Face API for root cause analysis.\"\"\"\n",
    "    api_token = os.getenv(\"HUGGING_FACE_API_TOKEN\", \"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\")\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "    payload = {\"inputs\": f\"Analyze anomaly in this data: {row.to_dict()}\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\", json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[0][\"generated_text\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ Error during API call: {e}\")\n",
    "        return \"No insight available\"\n",
    "\n",
    "# ============== 7. AUTOMATED CORRECTIVE ACTIONS USING AGENTIC AI ==============\n",
    "def suggest_corrective_action(row):\n",
    "    \"\"\"Uses Hugging Face API to generate corrective actions for anomalies.\"\"\"\n",
    "    api_token = os.getenv(\"HUGGING_FACE_API_TOKEN\", \"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\")\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "    payload = {\"inputs\": f\"Suggest corrective action for this anomaly: {row.to_dict()}\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\", json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[0][\"generated_text\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ Error during API call: {e}\")\n",
    "        return \"No corrective action available\"\n",
    "\n",
    "# ============== 8. MAIN EXECUTION ==============\n",
    "file_path = \"uploaded_data.csv\"\n",
    "scaled_data = preprocess_new_data(file_path, trained_features, scaler)\n",
    "\n",
    "if scaled_data is not None:\n",
    "    df_user = load_user_data(file_path)\n",
    "    df_user[\"Anomaly\"] = predict_anomalies(scaled_data)\n",
    "    df_user[\"Root_Cause_Analysis\"] = df_user.apply(lambda row: analyze_root_cause(row) if row[\"Anomaly\"] else \"Normal\", axis=1)\n",
    "    df_user[\"Corrective_Action\"] = df_user.apply(lambda row: suggest_corrective_action(row) if row[\"Anomaly\"] else \"None\", axis=1)\n",
    "    \n",
    "    df_user.to_csv(\"anomaly_results.csv\", index=False)\n",
    "    print(f\"✅ Anomaly detection & corrective actions completed! Results saved to 'anomaly_results.csv'.\")\n",
    "    print(df_user[[\"Anomaly\", \"Root_Cause_Analysis\", \"Corrective_Action\"]].head(10))\n",
    "else:\n",
    "    print(\"❌ Process failed due to preprocessing errors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c090e4-1ddd-4547-a456-34a1d623eeee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
